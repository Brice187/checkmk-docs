include::global_attr.adoc[]
= The Piggyback mechanism
:revdate: draft
:title: The Piggyback mechanism
:description: 

{related-start}
link:wato_monitoringagents.html[Monitoring agents]
link:monitoring_azure.html[Monitoring Microsoft Azure]
link:monitoring_aws.html[Monitoring Amazon Web Services (AWS)]
link:monitoring_kubernetes.html[Monitoring Kubernetes]
link:monitoring_docker.html[Monitoring Docker]
link:monitoring_vmware.html[Monitoring VMWare ESXi]
{related-end}

== Introduction

The piggyback mechanism was already in place in the early days of {CMK} -- as a
part of the monitoring of link:monitoring_vmware.html[VMware].
Here is a situation in which data needs to be queried from a particular host
because the data is located only on that host (for example, from an ESX host
system or the vCenter), but in the monitoring the data relates to a completely
different host (a virtual machine for example).

This cannot be realized with the normal mechanism in {CMK} because this
automatically assigns data and services that it fetches from a host. It would
also be very impractical for the operation if all information for all VMs always
appeared directly at the ESX host or even the vCenter.

The term ‘piggyback’ describes the process by which monitoring data for host B
is _piggybacked_ (so to speak) with the data queried from host A.

These days piggyback is used in many other monitoring plug-ins,
e.g. in monitoring link:monitoring_aws.html[AWS], link:monitoring_azure.html[Azure],
link:monitoring_kubernetes.html[Kubernetes], or link:monitoring_docker.html[Docker].
It is also very easy to use the piggyback mechanism yourself if you want to
implement your own check plug-ins in which you want to transfer data from one
source to any other host(s).


== The Piggyback principle

The basic principle of piggyback works as shown in the following diagram.
Host _A_ not only has its own monitoring data, but also that from other
hosts -- or more generally from other objects.
For example, an ESX host records the state and many current metrics for each of
its VMs. This host _A_ is sometimes referred to as the _source host_
in this context.

If {CMK} now retrieves the monitoring data from _A_ in its regular
one-minute intervals -- be it from the normal {CMK} agent or from a special
agent via a manufacturer's API -- in the response it also receives
specially-marked reporting data from the other hosts/objects _B_, _C_,
and so on. This _piggyback data_ is then placed in files on the {CMK}
server for later processing. The hosts _B_, _C_, and so on are
referred to as _Piggybacked Hosts_.

If {CMK} later requires the monitoring data from _B_ or _C_,
they are already in the local files and can be processed directly without having
to query an agent:

image::piggyback_scheme_1.png[width=50%]

It is also possible and useful to combine normal monitoring and piggyback.
Let's take the example of VMware again: You may have installed a {CMK} agent in
your VM _B_ which evaluates local information from the VM that is not known
to the ESX host (e.g., processes running in the VM).
In this case _not only_ will the agent be queried, _but_ its data will
_also_ be combined with the piggyback data received from host _A_:

image::piggyback_scheme_2.png[width=38%]


== Piggyback in practice

=== Setting up piggyback

First the good news -- The piggyback mechanism works completely automatically:
* If piggyback data for other hosts is detected when querying _A_ they are automatically saved for later evaluation.
* If piggyback data from another host is found when querying _B_ it will be used automatically.

However -- as usual in {CMK} -- everything is configurable.
Namely, in the properties of a host (such as host _B _) in the
[.guihint]#Data Sources# box you can set how it should react to existing or missing
piggyback data:

image::piggyback_settings.png[]

The default is [.guihint]#Use piggyback data from other hosts if present#.
If available, piggyback data is used, and if none is there the host just uses
its ‘own’ monitoring data.

With the [.guihint]#Always use and expect piggback data# setting you _force_ the
processing of piggyback data. If the data is missing or outdated the
[.guihint]#Check_MK# service will issue a warning.

And with [.guihint]#Never use piggyback data# any piggyback data found is simply
ignored -- a setting that you'll only need in exceptional cases.


=== Hosts must be present

Of course for a host to process piggyback data the host itself must be present
in the monitoring. In the example of ESX this means that you must also have your
VMs as hosts in {CMK} so that they are actually monitored.

Starting with Version {v16} of the {EE}, using the
link:dcd.html[dynamic configuration] you can also automate this and automatically create
hosts for which piggback data is available.


=== Hostnames and their assignments

In the above schemes it was somehow logical that the data from object _B_
was assigned to host _B_ in the monitoring. But what _exactly_ is B?

With the piggyback mechanism the assignment always uses a _name_.
The (special) agent writes an object name for each set of piggyback data.
In the case of ESX, e.g. the name of the virtual machine.
Some plug-ins -- such as link:monitoring_docker.html[docker] -- also have several options
for what should be used as a name.

For the mapping to work correctly, the name of the matching host in {CMK} must
of course be identical -- including upper and lower case.

But what happens if the names of objects in the piggyback data are inappropriate
or undesirable for monitoring? There is the special
[.guihint]#Access to Agents > General Settings > Hostname translation for piggybacked hosts#
link:wato_rules.html[rule set] for such situations.

To configure a rename you need to do two things:

. Create a rule in this rule chain and set the condition to access the _source host_ – ie. host _A_.
. Create a suitable name assignment value in the rule.

Here is an example of the value in a rule.
Two things are configured: first, all host names from the piggyback data are
converted to lowercase letters.
Then the two hosts `mv0815` or `vm0816` are also converted to the
{CMK} host's `mylnxserver07` or `mylnxserver08` :

image::piggyback_hostname_translation.png[]

More flexible is the method using link:regexes.html[regular expressions] found under
[.guihint]#Multiple regular expressions#.
This is useful if the renaming of many hosts is necessary, and it is done
according to a specific scheme.
Proceed as follows:

. Activate the [.guihint]#Multiple regular expressions# option.
. Add a translation entry with the [.guihint]#Add expression# button -- two fields will appear.
. In the first field -- [.guihint]#Regular expression# -- enter a regular expression that matches the original object name and which contains at least one subgroup – that is, a subexpression enclosed in parentheses. For a good explanation of these groups link:regexes.html#matchgroups[see the article on regular expressions].
. In [.guihint]#Replacement# specify a schema for the desired target host name in which the values that were ‘trapped’ with the subgroups will be replaced by `\1`, `\2`, etc.

An example of a regular expression would be, for example `vm(pass:[.*])-local`.
The substitute value `myvm\1` would then translate the name
`vmharri-local` into `myvmharri`.


== The technology behind this process

=== Transport of the piggyback data

As described above the piggyback data is also transported to other hosts with
the agent output from the ‘source host’.
The output from the {CMK} agent is a simple text-based format which is shown in
link:wato_monitoringagents.html[the article on monitoring agents].

What’s new is that a line is allowed in the output that starts with
`&lt;&lt;&lt;&lt;` and ends with `&gt;&gt;&gt;&gt;`.
In between is a hostname. All further monitoring data starting from this line is
then assigned to this host.
Here is an example excerpt that assigns the section
`+<<<esx_vsphere_vm>>>+` to the host `316-VM-MGM`:

[{file}]
----
<<<<316-VM-MGM>>>>
<<<esx_vsphere_vm>>>
config.datastoreUrl url /vmfs/volumes/55b643e1-3f344a10-68eb-90b11c00ff94|uncommitted 12472944334|name EQLSAS-DS-04|type VMFS|accessible true|capacity 1099243192320|freeSpace 620699320320
config.hardware.memoryMB 4096
config.hardware.numCPU 2
config.hardware.numCoresPerSocket 2
guest.toolsVersion 9537
guest.toolsVersionStatus guestToolsCurrent
guestHeartbeatStatus green
name 316-VM-MGM
----

A line with the content `&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;` can be used to
end this assignment. Any further output then belongs again to the source host.

When processing the agent output CMK extracts the parts intended for other hosts
and places them in files under `tmp/check_mk/piggyback`.
Below this is a subdirectory for each target host
(for example, for each VM) -- that is if we stick to our example with the
name `B`.
In this subdirectory there will then be a separate file with the actual data
from each source host. Their names would be `A` in our example.
Why is this so complicated? Well -- one host can indeed get piggyback data from
_multiple_ hosts, so a single file would not be enough.

*Tip:* If you are curious about what the piggyback data looks like,
look up the agent output from your monitoring instance’s hosts in the
`tmp/check_mk/cache` directory. An overview of all involved files and
directories can be found link:piggyback.html#files[below].


=== Orphaned piggyback data

If you cannot or do not want to use the link:dcd.html[dynamic host configuration],
you may receive piggyback data from a host that you have not even created in
{CMK}. This may be intentional, but it may also be an error -- e.g. because a
name is not an exact match.

In the ‘Treasures’ section you will find a script called
`find_piggy_orphans` with which your {CMK} can search for piggyback data
for which there is no host in monitoring.
Simply call this script without any arguments.
The script will output a list with one line -- sorted by name - for each
non-monitored piggy-host found:

[{shell}]
----
{c-omd} share/doc/check_mk/treasures/find_piggy_orphans
fooVM01
barVM02
----

This output is ‘clean’ and can, for example, be processed in a script.


=== Piggyback in distributed environments

Please note that in link:distributed_monitoring.html[distributed environments] the
current situation is that the source host and the piggybacked hosts must be
monitored in the same instance. This is simply because -- for efficiency
reasons -- the transmission of data between the hosts is done by using local
file exchange running via the `tmp/check_mk` directory.

Future versions of {CMK} may provide a mechanism allowing the optional
transmission of piggyback data across instance boundaries.


// 
//
// 
//
// 
//
// 
//
// 


[#files]
== Files and directories

=== File paths on the {CMK} server

[cols="35,~"]
|===
|Path |Description 

|tmp/check_mk/piggyback/ |Storage location for piggyback data
|tmp/check_mk/piggyback/B/ |Directory for piggyback data _for_ Host B
|tmp/check_mk/piggyback/B/A |File with piggyback data _from_ Host A _for_ Host B
|tmp/check_mk/piggyback_sources/ |Meta information for the hosts creating piggyback data
|tmp/check_mk/cache/A |Agent output from Host A -- including any existing piggyback data in a raw-format
|===

